# ğŸ  House Price Prediction using Machine Learning 

## ğŸ“Œ Overview

This project implements an end-to-end machine learning regression pipeline to predict house prices using the Ames Housing dataset.
It demonstrates the complete ML workflow, from data preprocessing and feature engineering to model training and evaluation.
The model predicts house prices (`SalePrice`) based on multiple structural and neighborhood-level features.

## ğŸ¯ Objectives

* Perform exploratory data analysis (EDA)
* Handle missing values and skewed data
* Apply feature engineering
* Train regression models
* Evaluate performance using standard metrics
* Generate predictions for unseen data

## ğŸ“Š Dataset

House Prices â€” Advanced Regression Techniques (Kaggle)

https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data

Features include: Property size and living area, Number of rooms, Year built, Garage and basement details, Neighborhood information

Target Variable: SalePrice

## ğŸ›  Tech Stack

**Language**
* Python

**Libraries**
* Pandas
* NumPy
* Matplotlib
* Seaborn
* Scikit-learn
* XGBoost

**Environment**
* Jupyter Notebook

## âš™ï¸ Workflow

* Load training and test datasets
* Perform exploratory data analysis (EDA)
* Apply log transformation to target variable
* Handle missing values
* Perform feature engineering
* Apply One-Hot Encoding to categorical variables
* Split dataset into train and validation sets
* Scale numerical features using StandardScaler
* Train Linear Regression and XGBoost Regressor
* Evaluate models using RMSE, MAE, and RÂ²
* Generate predictions on test data

## ğŸ¤– Models Used

#### * Linear Regression
Baseline regression model used for comparison.

#### * XGBoost Regressor
Gradient boosting model capable of capturing complex feature relationships and improving prediction accuracy.

## ğŸ“ˆ Results

The XGBoost model outperformed Linear Regression, achieving lower prediction error and better generalization.

Evaluation Metrics
* RMSE
* MAE
* RÂ² Score

## ğŸš€ Future Improvements

* Hyperparameter tuning
* Cross-validation pipeline
* Ensemble learning methods
* Model deployment with Streamlit
* Feature importance visualization
* Automated feature selection
